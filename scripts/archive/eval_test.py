#!/usr/bin/env python3
"""
æµ‹è¯•é›†è¯„ä¼°è„šæœ¬
ä¸“é—¨ç”¨äºåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
"""

from argparse import ArgumentParser
import pytorch_lightning as pl
from torch_geometric.loader import DataLoader
from smart.datasets.scalable_dataset import MultiDataset
from smart.datasets.maritime_dataset import MaritimeDataset
from smart.model import SMART
from smart.transforms import WaymoTargetBuilder, MaritimeTargetBuilder
from smart.utils.config import load_config_act
from smart.utils.log import Logging

if __name__ == '__main__':
    pl.seed_everything(2, workers=True)
    parser = ArgumentParser()
    parser.add_argument('--config', type=str, default="configs/train/train_maritime.yaml")
    parser.add_argument('--pretrain_ckpt', type=str, required=True, help="Path to checkpoint file")
    parser.add_argument('--split', type=str, default='test', choices=['val', 'test'], help="Which split to evaluate on")
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸ” SMART Maritime æ¨¡å‹è¯„ä¼°")
    print("="*80)
    print(f"ğŸ“‚ é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ’¾ æ¨¡å‹æ£€æŸ¥ç‚¹: {args.pretrain_ckpt}")
    print(f"ğŸ“Š è¯„ä¼°æ•°æ®é›†: {args.split}")
    print("="*80)
    
    config = load_config_act(args.config)
    data_config = config.Dataset
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ä¸åŒçš„Datasetå’ŒTransform
    dataset_classes = {
        "scalable": MultiDataset,
        "maritime": MaritimeDataset,
    }
    
    transform_classes = {
        "scalable": WaymoTargetBuilder,
        "maritime": MaritimeTargetBuilder,
    }
    
    dataset_class = dataset_classes[data_config.dataset]
    transform_class = transform_classes[data_config.dataset]
    
    # æ ¹æ®splité€‰æ‹©æ•°æ®ç›®å½•
    if args.split == 'test':
        raw_dir = data_config.test_raw_dir
        processed_dir = data_config.test_processed_dir
    else:
        raw_dir = data_config.val_raw_dir
        processed_dir = data_config.val_processed_dir
    
    print(f"\nğŸ“ åŠ è½½{args.split}æ•°æ®é›†...")
    eval_dataset = dataset_class(
        root=data_config.root, 
        split=args.split,
        raw_dir=raw_dir,
        processed_dir=processed_dir,
        transform=transform_class(config.Model.num_historical_steps, config.Model.decoder.num_future_steps)
    )
    
    print(f"   æ•°æ®é›†å¤§å°: {len(eval_dataset)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºDataLoader
    dataloader = DataLoader(
        eval_dataset, 
        batch_size=data_config.val_batch_size if hasattr(data_config, 'val_batch_size') else data_config.batch_size,
        shuffle=False, 
        num_workers=data_config.num_workers,
        pin_memory=data_config.pin_memory, 
        persistent_workers=True if data_config.num_workers > 0 else False
    )
    
    print(f"\nğŸ§  åŠ è½½æ¨¡å‹...")
    logger = Logging().log(level='INFO')
    model = SMART(config.Model)
    model.inference_token = True  # å¯ç”¨æ¨ç†åº¦é‡ï¼ˆminADE/minFDEï¼‰
    model.load_params_from_file(filename=args.pretrain_ckpt, logger=logger)
    
    # åˆ›å»ºTrainerå¹¶è¯„ä¼°
    print(f"\nâš¡ å¼€å§‹è¯„ä¼°...")
    trainer_config = config.Trainer
    trainer = pl.Trainer(
        accelerator=trainer_config.accelerator,
        devices=trainer_config.devices,
        strategy='ddp_find_unused_parameters_false', 
        num_sanity_val_steps=0
    )
    
    results = trainer.validate(model, dataloader)
    
    print("\n" + "="*80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*80)
    
    if results:
        print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
        for key, value in results[0].items():
            print(f"   {key}: {value:.4f}")
    
    print("\n" + "="*80)

