#!/usr/bin/env python3
"""
Maritime Trajectory Clustering Script
ä¸ºæµ·ä¸Šåœºæ™¯åˆ›å»ºè½¨è¿¹è¯æ±‡è¡¨ï¼ˆTrajectory Vocabularyï¼‰
"""

import os
import sys
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/mahexing/SMART-main')

from smart.utils.geometry import wrap_angle


def wrap_angle_np(angle, min_val=-np.pi, max_val=np.pi):
    """å°†è§’åº¦åŒ…è£¹åˆ°[min_val, max_val)ï¼ˆNumPy ç‰ˆï¼‰ã€‚"""
    return min_val + (angle + max_val) % (max_val - min_val)


def normalize_trajectories(trajectories):
    """
    å°†è½¨è¿¹å½’ä¸€åŒ–åˆ°ç›¸å¯¹èµ·ç‚¹/èµ·å§‹æœå‘çš„åæ ‡ç³»ã€‚

    Args:
        trajectories: [N, shift+1, 3]ï¼Œ(x, y, theta)

    Returns:
        traj_norm: [N, shift+1, 3]
    """
    traj = trajectories.copy()
    # 1) ç›¸å¯¹èµ·ç‚¹å¹³ç§»
    traj[:, :, 0:2] = traj[:, :, 0:2] - traj[:, 0:1, 0:2]
    # 2) æ—‹è½¬åˆ°é¦–å¸§æœå‘çš„å±€éƒ¨åæ ‡ï¼ˆç»• -theta0ï¼‰
    theta0 = traj[:, 0:1, 2]
    cos0 = np.cos(theta0)
    sin0 = np.sin(theta0)
    x = traj[:, :, 0]
    y = traj[:, :, 1]
    x_r =  cos0 * x + sin0 * y
    y_r = -sin0 * x + cos0 * y
    traj[:, :, 0] = x_r
    traj[:, :, 1] = y_r
    # 3) èˆªå‘ç›¸å¯¹åŒ–å¹¶åšè§’åº¦åŒ…è£¹
    traj[:, :, 2] = wrap_angle_np(traj[:, :, 2] - theta0)
    return traj


def average_distance_vectorized(point_set1, centroids):
    """è®¡ç®—è½¨è¿¹åˆ°èšç±»ä¸­å¿ƒçš„å¹³å‡è·ç¦»ï¼ˆå‘é‡åŒ–ï¼‰"""
    dists = np.sqrt(np.sum((point_set1[:, None, :, :] - centroids[None, :, :, :])**2, axis=-1))
    return np.mean(dists, axis=2)


def assign_clusters(sub_X, centroids):
    """å°†è½¨è¿¹åˆ†é…åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ"""
    distances = average_distance_vectorized(sub_X, centroids)
    return np.argmin(distances, axis=1)


def Kdisk_cluster(X, N=256, tol=0.1, width=0, length=0, a_pos=None, 
                  x_min=-50, x_max=100, y_min=-50, y_max=50, cal_mean_heading=True):
    """
    K-disk èšç±»ç®—æ³•ï¼ˆé€‚é…æµ·ä¸Šåœºæ™¯ï¼‰
    
    Args:
        X: è½¨è¿¹å¤šè¾¹å½¢è½®å»“ [num_traj, 4, 2]
        N: ç›®æ ‡èšç±»æ•°é‡
        tol: å®¹å·®ï¼ˆæ§åˆ¶èšç±»ç´§å¯†ç¨‹åº¦ï¼‰
        width: èˆ¹èˆ¶å®½åº¦
        length: èˆ¹èˆ¶é•¿åº¦
        a_pos: åŸå§‹è½¨è¿¹æ•°æ® [num_traj, time_steps, 3] (x, y, theta)
        x_min, x_max, y_min, y_max: æœ‰æ•ˆåŒºåŸŸè¾¹ç•Œ
        cal_mean_heading: æ˜¯å¦è®¡ç®—å¹³å‡èˆªå‘
    
    Returns:
        centroids: èšç±»ä¸­å¿ƒ [N, 4, 2]
        ret_traj: ä»£è¡¨æ€§è½¨è¿¹ [N, time_steps, 3]
    """
    S = []
    ret_traj_list = []
    iteration = 0
    max_iterations = N * 10  # é˜²æ­¢æ— é™å¾ªç¯
    
    print(f"å¼€å§‹èšç±»: ç›®æ ‡{N}ä¸ªç°‡, å®¹å·®={tol:.3f}, èˆ¹èˆ¶å°ºå¯¸={width:.1f}x{length:.1f}m")
    
    while len(S) < N and iteration < max_iterations:
        iteration += 1
        num_all = X.shape[0]
        
        if num_all == 0:
            print(f"âš ï¸ è­¦å‘Š: å‰©ä½™è½¨è¿¹æ•°ä¸º0ï¼Œå·²è·å¾—{len(S)}ä¸ªèšç±»ä¸­å¿ƒ")
            break
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªè½¨è¿¹ä½œä¸ºå€™é€‰ä¸­å¿ƒ
        choice_index = np.random.choice(num_all)
        x0 = X[choice_index]
        
        # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿ä¸­å¿ƒç‚¹åœ¨åˆç†èŒƒå›´å†…
        center_x = x0[0, 0]
        center_y = x0[0, 1]
        if center_x < x_min or center_x > x_max or center_y < y_min or center_y > y_max:
            continue
        
        # è®¡ç®—è·ç¦»å¹¶åˆ†ç±»
        distances = np.sum((X - x0)**2, axis=(1, 2)) / 4
        res_mask = distances > (tol**2)  # ä¿ç•™çš„è½¨è¿¹
        del_mask = distances <= (tol**2)  # åˆ é™¤çš„è½¨è¿¹ï¼ˆå±äºå½“å‰ç°‡ï¼‰
        
        if cal_mean_heading and del_mask.sum() > 0:
            # è®¡ç®—ç°‡å†…è½¨è¿¹çš„å¹³å‡èˆªå‘
            del_contour = X[del_mask]
            diff_xy = del_contour[:, 0, :] - del_contour[:, 3, :]
            del_heading = np.arctan2(diff_xy[:, 1], diff_xy[:, 0]).mean()
            
            # ä½¿ç”¨å¹³å‡èˆªå‘é‡æ–°è®¡ç®—å¤šè¾¹å½¢è½®å»“
            x0_center_x = x0.mean(0)[0]
            x0_center_y = x0.mean(0)[1]
            x0 = cal_polygon_contour(x0_center_x, x0_center_y, del_heading, width, length)
            
            # ä½¿ç”¨ç°‡å†…è½¨è¿¹çš„å¹³å‡è½¨è¿¹ä½œä¸ºä»£è¡¨
            del_traj = a_pos[del_mask]
            ret_traj = del_traj.mean(0)[None, ...]
        else:
            x0 = x0[None, ...]
            ret_traj = a_pos[choice_index][None, ...]
        
        # æ›´æ–°æ•°æ®é›†ï¼ˆç§»é™¤å·²èšç±»çš„è½¨è¿¹ï¼‰
        X = X[res_mask]
        a_pos = a_pos[res_mask]
        
        S.append(x0)
        ret_traj_list.append(ret_traj)
        
        if len(S) % 50 == 0:
            print(f"  è¿›åº¦: {len(S)}/{N} ä¸ªèšç±»ä¸­å¿ƒ, å‰©ä½™è½¨è¿¹: {X.shape[0]}")
    
    if len(S) < N:
        print(f"âš ï¸ è­¦å‘Š: ä»…è·å¾—{len(S)}/{N}ä¸ªèšç±»ä¸­å¿ƒï¼ˆå¯èƒ½éœ€è¦å‡å°tolæˆ–å¢åŠ æ•°æ®é‡ï¼‰")
    
    centroids = np.concatenate(S, axis=0)
    ret_traj = np.concatenate(ret_traj_list, axis=0)
    
    return centroids, ret_traj


def cal_polygon_contour(x, y, theta, width, length):
    """
    è®¡ç®—çŸ©å½¢èˆ¹èˆ¶çš„å››ä¸ªè§’ç‚¹åæ ‡
    
    Args:
        x, y: èˆ¹èˆ¶ä¸­å¿ƒåæ ‡
        theta: èˆªå‘è§’ï¼ˆå¼§åº¦ï¼‰
        width: èˆ¹å®½
        length: èˆ¹é•¿
    
    Returns:
        polygon_contour: [4, 2] å››ä¸ªè§’ç‚¹ [å·¦å‰, å³å‰, å³å, å·¦å]
    """
    # å·¦å‰è§’
    left_front_x = x + 0.5 * length * np.cos(theta) - 0.5 * width * np.sin(theta)
    left_front_y = y + 0.5 * length * np.sin(theta) + 0.5 * width * np.cos(theta)
    left_front = np.column_stack((left_front_x, left_front_y))
    
    # å³å‰è§’
    right_front_x = x + 0.5 * length * np.cos(theta) + 0.5 * width * np.sin(theta)
    right_front_y = y + 0.5 * length * np.sin(theta) - 0.5 * width * np.cos(theta)
    right_front = np.column_stack((right_front_x, right_front_y))
    
    # å³åè§’
    right_back_x = x - 0.5 * length * np.cos(theta) + 0.5 * width * np.sin(theta)
    right_back_y = y - 0.5 * length * np.sin(theta) - 0.5 * width * np.cos(theta)
    right_back = np.column_stack((right_back_x, right_back_y))
    
    # å·¦åè§’
    left_back_x = x - 0.5 * length * np.cos(theta) - 0.5 * width * np.sin(theta)
    left_back_y = y - 0.5 * length * np.sin(theta) + 0.5 * width * np.cos(theta)
    left_back = np.column_stack((left_back_x, left_back_y))
    
    polygon_contour = np.concatenate((
        left_front[:, None, :], 
        right_front[:, None, :], 
        right_back[:, None, :], 
        left_back[:, None, :]
    ), axis=1)
    
    return polygon_contour


def load_maritime_trajectories(data_dirs, max_samples=100000, shift=1):
    """
    ä»maritimeæ•°æ®é›†ä¸­åŠ è½½è½¨è¿¹æ•°æ®
    
    Args:
        data_dirs: æ•°æ®ç›®å½•åˆ—è¡¨
        max_samples: æœ€å¤§åŠ è½½æ ·æœ¬æ•°
        shift: çª—å£è·¨åº¦ï¼ˆç”¨äºè¿åŠ¨tokenï¼Œ1 è¡¨ç¤ºæå–ä¸¤å¸§ [t, t+1]ï¼‰
    
    Returns:
        trajectories: [N, shift+1, 3] (x, y, theta)
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š ä»Maritimeæ•°æ®é›†åŠ è½½è½¨è¿¹")
    print("=" * 70)
    
    all_trajectories = []
    sample_count = 0
    
    for data_dir in data_dirs:
        data_dir = Path(data_dir)
        if not data_dir.exists():
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            continue
        
        # è·å–æ‰€æœ‰.ptæ–‡ä»¶
        pt_files = sorted(list(data_dir.glob("*.pt")))
        print(f"\nç›®å½•: {data_dir}")
        print(f"æ‰¾åˆ° {len(pt_files)} ä¸ªæ–‡ä»¶")
        
        for pt_file in tqdm(pt_files, desc="åŠ è½½æ–‡ä»¶"):
            try:
                # åŠ è½½æ–‡ä»¶
                data_list = torch.load(pt_file, map_location='cpu', weights_only=False)
                
                if not isinstance(data_list, list):
                    data_list = [data_list]
                
                # éå†æ–‡ä»¶ä¸­çš„æ¯ä¸ªåœºæ™¯
                for scene_data in data_list:
                    if sample_count >= max_samples:
                        break
                    
                    # éªŒè¯æ•°æ®æ ¼å¼
                    if not hasattr(scene_data, 'node_types') or 'agent' not in scene_data.node_types:
                        continue
                    
                    # è·å–agentèŠ‚ç‚¹æ•°æ®
                    agent_data = scene_data['agent']
                    
                    # x: [N_ships, T_steps, F_features]
                    # æˆ‘ä»¬éœ€è¦: x (0), y (1), theta (6)
                    if not hasattr(agent_data, 'x'):
                        continue
                    
                    features = agent_data.x  # [N_ships, T_steps, F_features]
                    
                    # æ£€æŸ¥å½¢çŠ¶
                    if features.dim() != 3 or features.shape[2] < 7:
                        continue
                    
                    N_ships = features.shape[0]
                    T_steps = features.shape[1]
                    
                    # æå–æ¯è‰˜èˆ¹çš„çŸ­ç¨‹2æ­¥è½¨è¿¹ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
                    for ship_idx in range(N_ships):
                        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ—¶é—´æ­¥
                        if T_steps < shift + 1:
                            continue
                        
                        # éå†æ‰€æœ‰èµ·ç‚¹ tï¼Œæ”¶é›† [t, t+shift] ä¸¤å¸§ç‰‡æ®µ
                        for t in range(0, T_steps - shift):
                            end_idx = t + shift + 1
                            # å¯é€‰ï¼šè‹¥å­˜åœ¨ valid_maskï¼Œè¦æ±‚çª—å£å†…å‡æœ‰æ•ˆ
                            vmask_ok = True
                            if hasattr(agent_data, 'valid_mask'):
                                vmask_seg = agent_data.valid_mask[ship_idx, t:end_idx].cpu().numpy()
                                vmask_ok = bool(vmask_seg.all())
                            if not vmask_ok:
                                continue
                            
                            # ç”¨é€Ÿåº¦ç§¯åˆ†é‡å»ºç›¸å¯¹ä½ç§»ï¼ˆé€Ÿåº¦æ›´å¯é ï¼‰
                            # ç‰¹å¾ç´¢å¼•: [x, y, vx, vy, ax, ay, theta, omega]
                            vx = features[ship_idx, t:end_idx, 2].numpy()
                            vy = features[ship_idx, t:end_idx, 3].numpy()
                            theta = features[ship_idx, t:end_idx, 6].numpy()
                            
                            x = np.zeros(shift + 1)
                            y = np.zeros(shift + 1)
                            x[0] = 0.0
                            y[0] = 0.0
                            dt = 30.0
                            for i in range(1, shift + 1):
                                x[i] = x[i-1] + vx[i-1] * dt
                                y[i] = y[i-1] + vy[i-1] * dt
                            
                            # ç•¥è¿‡å…¨é›¶ç‰‡æ®µ
                            if np.all(x == 0) and np.all(y == 0):
                                continue
                            
                            trajectory = np.stack([x, y, theta], axis=1)  # [shift+1, 3]
                            all_trajectories.append(trajectory)
                            sample_count += 1
                            if sample_count >= max_samples:
                                break
                        if sample_count >= max_samples:
                            break
                    
                    if sample_count >= max_samples:
                        break
                        
            except Exception as e:
                print(f"\nâš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥ {pt_file.name}: {e}")
                continue
            
            if sample_count >= max_samples:
                break
        
        if sample_count >= max_samples:
            break
    
    if len(all_trajectories) == 0:
        raise ValueError("æœªèƒ½åŠ è½½ä»»ä½•è½¨è¿¹æ•°æ®ï¼")
    
    trajectories = np.stack(all_trajectories, axis=0)  # [N, shift+1, 3]
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(trajectories)} æ¡è½¨è¿¹")
    print(f"   å½¢çŠ¶: {trajectories.shape}")
    print(f"   XèŒƒå›´: [{trajectories[:, :, 0].min():.2f}, {trajectories[:, :, 0].max():.2f}]")
    print(f"   YèŒƒå›´: [{trajectories[:, :, 1].min():.2f}, {trajectories[:, :, 1].max():.2f}]")
    print(f"   ThetaèŒƒå›´: [{trajectories[:, :, 2].min():.2f}, {trajectories[:, :, 2].max():.2f}]")
    
    return trajectories


def create_maritime_vocabulary(data_dirs, 
                               num_clusters=256,
                               shift=1,
                               max_samples=100000,
                               ship_width=10.0,
                               ship_length=50.0,
                               tolerance=0.15,
                               w_ang=100.0,
                               output_path='data/maritime_motion_vocab.pt'):
    """
    ä¸ºæµ·ä¸Šåœºæ™¯åˆ›å»ºè½¨è¿¹è¯æ±‡è¡¨
    
    Args:
        data_dirs: æ•°æ®ç›®å½•åˆ—è¡¨
        num_clusters: è¯æ±‡è¡¨å¤§å°ï¼ˆèšç±»æ•°é‡ï¼‰
        shift: çª—å£è·¨åº¦ï¼ˆç”Ÿæˆ shift+1 ä¸ªæ—¶é—´ç‚¹ï¼›1 è¡¨ç¤ºä¸¤å¸§ç‰‡æ®µï¼‰
        max_samples: æœ€å¤§ä½¿ç”¨æ ·æœ¬æ•°
        ship_width: èˆ¹èˆ¶å®½åº¦ï¼ˆç±³ï¼‰
        ship_length: èˆ¹èˆ¶é•¿åº¦ï¼ˆç±³ï¼‰
        tolerance: èšç±»å®¹å·®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "=" * 70)
    print("ğŸš¢ Maritimeè½¨è¿¹èšç±» - åˆ›å»ºè¿åŠ¨è¯æ±‡è¡¨")
    print("=" * 70)
    print(f"å‚æ•°é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {num_clusters}")
    print(f"  æ—¶é—´æ­¥æ•°: {shift + 1}")
    print(f"  èˆ¹èˆ¶å°ºå¯¸: {ship_width}m x {ship_length}m")
    print(f"  èšç±»å®¹å·®: {tolerance}")
    print(f"  æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    print(f"  è§’åº¦æƒé‡ w_ang: {w_ang:.2f} (ç±³^2)")
    
    # 1. åŠ è½½è½¨è¿¹æ•°æ®
    trajectories = load_maritime_trajectories(data_dirs, max_samples, shift)
    
    # 2. éšæœºé‡‡æ ·ï¼ˆå¦‚æœæ•°æ®å¤ªå¤šï¼‰
    if trajectories.shape[0] > max_samples:
        print(f"\nğŸ“‰ é™é‡‡æ ·: {trajectories.shape[0]} -> {max_samples}")
        indices = np.random.choice(trajectories.shape[0], max_samples, replace=False)
        trajectories = trajectories[indices]
    
    # 3. å½’ä¸€åŒ–åˆ°ç›¸å¯¹èµ·ç‚¹/èµ·å§‹æœå‘ï¼ˆå«æ—‹è½¬å¯¹é½ï¼‰
    print("\nğŸ”„ å½’ä¸€åŒ–çŸ­ç¨‹ç‰‡æ®µåˆ°ç›¸å¯¹åæ ‡ç³»ï¼ˆèµ·ç‚¹/æœå‘å¯¹é½ï¼‰...")
    traj_norm = normalize_trajectories(trajectories)  # [N, shift+1, 3]

    # 4. å±•å¹³å¹¶æ‰§è¡Œ KMeansï¼ˆ2æ­¥çŸ­ç¨‹ç‰‡æ®µï¼‰
    print("\nğŸ¯ æ‰§è¡Œ KMeans èšç±»ï¼ˆ2æ­¥çŸ­ç¨‹ç‰‡æ®µï¼‰...")
    try:
        from sklearn.cluster import KMeans
    except Exception as e:
        print("âŒ éœ€è¦ scikit-learnï¼špip install scikit-learn")
        raise

    # 4.1 ç”¨ (x, y, sqrt(w_ang)*cosÎ¸, sqrt(w_ang)*sinÎ¸) ä½œä¸ºèšç±»ç‰¹å¾ï¼ˆç»Ÿä¸€åº¦é‡ï¼‰
    print("\nâš™ï¸  ç‰¹å¾è½¬æ¢: ä½¿ç”¨ (x, y, sqrt(w_ang)*cosÎ¸, sqrt(w_ang)*sinÎ¸) ...")
    N = traj_norm.shape[0]
    S = shift + 1
    x = traj_norm[..., 0]
    y = traj_norm[..., 1]
    theta = traj_norm[..., 2]
    cos_theta = np.cos(theta)
    sin_theta = np.sin(theta)
    sqrt_w = float(np.sqrt(w_ang))
    features_transformed = np.stack([x, y, sqrt_w * cos_theta, sqrt_w * sin_theta], axis=-1)  # [N, S, 4]
    data_for_cluster = features_transformed.reshape(N, S * 4).astype(np.float32)

    n_clusters_eff = min(num_clusters, data_for_cluster.shape[0])
    if n_clusters_eff <= 0:
        raise ValueError("æœ‰æ•ˆèšç±»æ•°ä¸º0ï¼Œè¯·æ£€æŸ¥æ•°æ®é‡æˆ–å‚æ•°è®¾ç½®")

    kmeans = KMeans(n_clusters=n_clusters_eff, n_init=10, random_state=0)
    kmeans.fit(data_for_cluster)
    centers = kmeans.cluster_centers_.astype(np.float32)    # [K, S*4]
    centers_reshaped = centers.reshape(n_clusters_eff, S, 4)
    x_c = centers_reshaped[..., 0]
    y_c = centers_reshaped[..., 1]
    cos_c_w = centers_reshaped[..., 2]
    sin_c_w = centers_reshaped[..., 3]
    # è¿˜åŸæœªåŠ æƒçš„ cos/sinï¼Œç”¨äºè®¡ç®—è§’åº¦
    cos_c = cos_c_w / sqrt_w
    sin_c = sin_c_w / sqrt_w
    theta_c = np.arctan2(sin_c, cos_c)
    ret_traj = np.stack([x_c, y_c, theta_c], axis=-1).astype(np.float32)  # [K, S, 3]
    # è§’åº¦è§„èŒƒåˆ° (-pi, pi]
    ret_traj[:, :, 2] = wrap_angle_np(ret_traj[:, :, 2])

    # ä»¥ä»£è¡¨æ€§è½¨è¿¹çš„æœ«æ—¶åˆ»è®¡ç®—å¤šè¾¹å½¢ä½œä¸º tokenï¼ˆä¸åŸç»“æ„ä¸€è‡´ï¼‰
    centroids = cal_polygon_contour(
        ret_traj[:, -1, 0],
        ret_traj[:, -1, 1],
        ret_traj[:, -1, 2],
        ship_width,
        ship_length
    )
    
    print(f"\nâœ… èšç±»å®Œæˆ!")
    print(f"   è·å¾— {centroids.shape[0]} ä¸ªèšç±»ä¸­å¿ƒ")
    print(f"   ä»£è¡¨æ€§è½¨è¿¹å½¢çŠ¶: {ret_traj.shape}")
    
    # 6. é‡æ–°è®¡ç®—å®Œæ•´æ—¶é—´åºåˆ—çš„å¤šè¾¹å½¢
    print("\nğŸ“Š è®¡ç®—å®Œæ•´æ—¶é—´åºåˆ—çš„å¤šè¾¹å½¢...")
    num_actual_clusters = ret_traj.shape[0]
    contour_all = cal_polygon_contour(
        ret_traj[:, :, 0].reshape(num_actual_clusters * (shift + 1)),
        ret_traj[:, :, 1].reshape(num_actual_clusters * (shift + 1)),
        ret_traj[:, :, 2].reshape(num_actual_clusters * (shift + 1)),
        ship_width,
        ship_length
    )
    contour_all = contour_all.reshape(num_actual_clusters, (shift + 1), 4, 2)
    
    # 7. ä¿å­˜è¯æ±‡è¡¨
    vocab = {
        'token': {'ship': centroids},  # [N, 4, 2] èšç±»ä¸­å¿ƒçš„å¤šè¾¹å½¢
        'traj': {'ship': ret_traj},    # [N, shift+1, 3] ä»£è¡¨æ€§è½¨è¿¹
        'token_all': {'ship': contour_all},  # [N, shift+1, 4, 2] å®Œæ•´æ—¶é—´åºåˆ—
        'metadata': {
            'num_clusters': num_actual_clusters,
            'shift': shift,
            'ship_width': ship_width,
            'ship_length': ship_length,
            'tolerance': tolerance,
            'num_samples': trajectories.shape[0],
            'method': 'kmeans_xy_cossin_weighted',
            'w_ang': float(w_ang)
        }
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜
    torch.save(vocab, output_path)
    print(f"\nğŸ’¾ è¯æ±‡è¡¨å·²ä¿å­˜: {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.2f} KB")
    
    # 8. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ğŸ“ˆ è¯æ±‡è¡¨ç»Ÿè®¡")
    print("=" * 70)
    print(f"è¯æ±‡è¡¨å¤§å°: {num_actual_clusters}")
    print(f"æ—¶é—´æ­¥æ•°: {shift + 1}")
    print(f"èˆ¹èˆ¶å°ºå¯¸: {ship_width}m x {ship_length}m")
    print(f"\nè½¨è¿¹ç»Ÿè®¡:")
    print(f"  Xä½ç§»èŒƒå›´: [{ret_traj[:, :, 0].min():.2f}, {ret_traj[:, :, 0].max():.2f}]")
    print(f"  Yä½ç§»èŒƒå›´: [{ret_traj[:, :, 1].min():.2f}, {ret_traj[:, :, 1].max():.2f}]")
    print(f"  èˆªå‘èŒƒå›´: [{ret_traj[:, :, 2].min():.2f}, {ret_traj[:, :, 2].max():.2f}]")
    
    return vocab


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Maritimeè½¨è¿¹èšç±» - åˆ›å»ºTokenè¯è¡¨')
    parser.add_argument('--data_dirs', type=str, nargs='+', 
                        default=['data/maritime_windows_30s_no_norm/train'],
                        help='è®­ç»ƒæ•°æ®ç›®å½•åˆ—è¡¨')
    parser.add_argument('--output', type=str, 
                        default='smart/tokens/maritime_tokens_no_norm.pkl',
                        help='è¾“å‡ºTokenè¯è¡¨è·¯å¾„')
    parser.add_argument('--token_size', type=int, default=2048,
                        help='Tokenè¯è¡¨å¤§å°ï¼ˆèšç±»æ•°é‡ï¼‰')
    parser.add_argument('--shift', type=int, default=1,
                        help='çª—å£è·¨åº¦ï¼ˆç”Ÿæˆ shift+1 ä¸ªæ—¶é—´ç‚¹ï¼›1 è¡¨ç¤ºä¸¤å¸§ç‰‡æ®µï¼‰')
    parser.add_argument('--max_samples', type=int, default=100000,
                        help='æœ€å¤§ä½¿ç”¨æ ·æœ¬æ•°')
    parser.add_argument('--ship_width', type=float, default=10.0,
                        help='èˆ¹èˆ¶å®½åº¦ï¼ˆç±³ï¼‰')
    parser.add_argument('--ship_length', type=float, default=50.0,
                        help='èˆ¹èˆ¶é•¿åº¦ï¼ˆç±³ï¼‰')
    parser.add_argument('--tolerance', type=float, default=0.1,
                        help='èšç±»å®¹å·®ï¼ˆè¶Šå°è¶Šç´§å¯†ï¼Œå»ºè®®0.08-0.15ï¼‰')
    parser.add_argument('--w_ang', type=float, default=100.0,
                        help='è§’åº¦æƒé‡ï¼ˆç±³^2ï¼‰ï¼Œç¡®å®š cos/sin å¯¹è·ç¦»çš„è´¡çŒ®')
    
    args = parser.parse_args()
    
    # é…ç½®å‚æ•°
    config = {
        'data_dirs': args.data_dirs,
        'num_clusters': args.token_size,
        'shift': args.shift,
        'max_samples': args.max_samples,
        'ship_width': args.ship_width,
        'ship_length': args.ship_length,
        'tolerance': args.tolerance,
        'w_ang': args.w_ang,
        'output_path': args.output
    }
    
    print("\n" + "=" * 70)
    print("ğŸš¢ Maritimeè½¨è¿¹èšç±»è„šæœ¬")
    print("=" * 70)
    print("\né…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºè¯æ±‡è¡¨
    vocab = create_maritime_vocabulary(**config)
    
    print("\n" + "=" * 70)
    print("âœ… å®Œæˆï¼")
    print("=" * 70)
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"1. æ£€æŸ¥ç”Ÿæˆçš„è¯æ±‡è¡¨: {config['output_path']}")
    print(f"2. éªŒè¯Tokenæ–¹å‘åˆ†å¸ƒï¼ˆåº”è¯¥å‡è¡¡ï¼‰")
    print(f"3. æ›´æ–°è®­ç»ƒé…ç½®æ–‡ä»¶")
    print(f"4. å¼€å§‹è®­ç»ƒæ¨¡å‹")

